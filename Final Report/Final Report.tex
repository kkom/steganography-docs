%%%
%%% Document class and page properties
%%%

\documentclass{IIBproject}

% Set margin size
\usepackage[margin=2.5cm]{geometry}

%%%
%%% Standard packages in alphabetical order
%%%

% Use BibTeX for bibliography management, use the hyperref package to include URLs in the entries
\usepackage{cite}
\usepackage{url}

% Color text
\usepackage{color}

% Sets 1.5 line spacing
\usepackage{setspace}
\onehalfspacing

% Provides formatting of numbers (here used for thousands' separators)
\usepackage{siunitx}

% Provides robust spaces after commands
\usepackage{xspace}

%%%
%%% Commands
%%%

% Placeholder for parts of the report to be finished later
\DeclareRobustCommand{\later}{\textcolor{red}{\textbf{(?)}}\@\xspace}

% Notes to be discussed with supervisor/thought about later
\DeclareRobustCommand{\noteSelf}[1]{\textcolor{red}{#1}}

%%%
%%% Document
%%%

\begin{document}

% Title page
\thispagestyle{empty}
\author{Konrad Komorowski (CHU)}
\title{Hiding Secrets in Plain Text}
\projectgroup{F}
\maketitle

% Summary and table of contents
\thispagestyle{empty}
\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}
\tableofcontents

% Contents
\pagestyle{plain}

\newpage
\section{Technical abstract}

Lorem ipsum.

\newpage
\section{Introduction}

Lorem ipsum.

\subsection{Aim}

\subsection{Overview of steganography}

\subsection{High level description of the stegosystem}

\newpage
\section{The interval algorithm}

Lorem ipsum.

\newpage
\section{Statistical language models}

\subsection{Standard theory and notation}

The notation and theory in this section is based on \cite{4f11:statistical_language_models, 4f11:smt_systems, coursera:nlp}.

\subsubsection{Text as the output of a stochastic process}

A string $\mathbf w$ of length $L$ can be regarded as a sequence of \emph{tokens} $w_l$. Tokens are atomic elements of a sentence such as words or punctuation.

\begin{equation}
\mathbf w = [ w_1, \dots, w_L ] \equiv w_1^L
\end{equation}

$\mathbf w$ can be regarded as an $L$-dimensional vector of jointly distributed random variables $w_l \in \{1, \dots, M\}$, where $M$ is the number of distinct tokens. According to the chain rule of probability, we can factorise $P(\mathbf w)$ in the following way:

\begin{equation}
\label{eq:exact_string_probability}
P(\mathbf w) = \prod_{l=1}^{L} P( w_l | w_1^{l-1} )
\end{equation}

\subsubsection{$N$-gram language models}

Even though Eq.~\ref{eq:exact_string_probability} gives us a convenient formulation of language as a stochastic process, it is computationally intractable. The conditional probabilities need to be stored in tables. Conditional probability table of just the last token would have $M^L$ entries. With $M \approx 10^6$ and $L \approx 10$, this number very quickly becomes prohibitively large.

The usual solution is to restrict the size of the context to $N-1$ previous tokens, resulting in an approximate $N$-gram language model:

\begin{equation}
\label{eq:ngram_string_probability}
P(\mathbf w) \approx P_{\text{$N$-gram}}(\mathbf w) \equiv \prod_{l=1}^{L} P( w_l | w_{l-N+1}^{l-1} )
\end{equation}

\subsubsection{Estimating parameters}

Eq.~\ref{eq:ml_conditional_ngram_probability} can be shown to be the ML (maximum likelihood) estimate of conditional probability of $w_l$ in an $N$-gram model. $f(\cdot)$ is the number of occurrences of a particular sequence of tokens in the training dataset.

\begin{equation}
\label{eq:ml_conditional_ngram_probability}
\hat P( w_l | w_{l-N+1}^{l-1} ) = \frac {f(w_{l-N+1}^l)} {f(w_{l-N+1}^{l-1})}
\end{equation}

\subsubsection{Discounting and back-off}

A problem with Eq.~\ref{eq:ml_conditional_ngram_probability} is that most of the conditional probabilities will be zero. For example, a $5$-gram model with $M \approx 10^6$ will still have $M^L \approx \left( 10^6 \right)^5 = 10^{30}$ parameters. For every possible $5$-gram sequence to occur at least once, the training set would need to consist of at least $\approx 10^{30}$ tokens. Currently largest available dataset is based on $\approx 3.6 \times 10^{11}$ English words. \cite{googlengrams2011} The standard way of dealing with this problem is by introducing \emph{discounting} and \emph{back-off} to the model.

\subsection{Sources of natural language statistical data}

\subsubsection{COCA}

\subsubsection{Google Books Ngrams}

\newpage
\section{Detailed description of the stegosystem}

Lorem ipsum.

\subsection{Source -- plaintext concatenated with randomness}

\subsection{Target -- English language model}

\subsection{Storing length of plaintext}

\subsubsection{Universal coding of plaintext length}

\subsubsection{EOF symbol}

\subsection{Sufficient length of the output interval}

\subsection{Secrecy}

\newpage
\section{Implementation}

Lorem ipsum.

\subsection{Language model}

\subsubsection{Mining the Google Books Ngrams Database}

\subsubsection{Efficient computation of conditional probabilities}

\subsection{Plaintext to stegotext translation using the interval algorithm}

\newpage
\section{Security vulnerabilities}

Lorem ipsum.

\subsection{Telling stegotext from covertext}

\subsubsection{Plaintext length declaration incompatible with interval size}

If the plaintext length declaration is longer than the maximum length of plaintext that can be recovered from the potential stegotext, we know the message was covertext.

Conversely -- how often does a compatible length declaration indicate stegotext? If almost always, then the system is flawed.

\newpage
\section{Theoretical considerations}

Lorem ipsum.

\subsection{Rate of the system}

\subsection{Entropy of the language model and its effect on stegotext length}

Use the bounds from Han \& Hoshi's paper.

\newpage
\section{Interesting questions}

Lorem ipsum.

\subsection{How many bits of information stored per word, sentence, tweet?}

\subsubsection{Dependence on the entropy of the language model}

\newpage
\footnotesize
\bibliographystyle{unsrt}
\bibliography{../bibliography}

\newpage
\appendix

\section{First appendix}

\end{document}